#!/usr/bin/env python

# -*- coding: utf-8 -*-
import argparse
import json
import os
import sys
from importlib import import_module

from pyjobs_crawlers.run import crawl_from_class_name, StdOutputConnector
from pyjobs_crawlers.run import stdout_error_callback

current_dir = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(current_dir, '../../'))

parser = argparse.ArgumentParser(description='Test spider command')
parser.add_argument('spider',
                    metavar='SPIDER',
                    type=str,
                    help='Spider classpath '
                         '(eg. pyjobs_crawlers.spiders.afpy.AfpyJobSpider).')
parser.add_argument('-d',
                    '--debug-crawling',
                    action='store_true',
                    default=False,
                    dest='debug_crawling',
                    help="Activate debugging operations for the spider. Must "
                         "be set in order to use any of the others "
                         "debugging options (such as -l, -r, -o, -s and -p). "
                         "Otherwise, this script will start the spider using "
                         "default non debugging parameters, ignoring every "
                         "debugging arguments that's been passed to it.")
parser.add_argument('-l',
                    '--job-list-url',
                    default=None,
                    dest='job_list_page_url',
                    help='An url pointing to a job list web page you want to '
                         'test against the job list parser of the specified '
                         'spider class (non recursive by default, see -r). '
                         'Cannot be used simultaneously with -p.')
parser.add_argument('-r',
                    '--recursive',
                    action='store_true',
                    default=False,
                    dest='recursive_crawling',
                    help='Add this argument in case you want the parsing of '
                         'job list pages to be recursive.')
parser.add_argument('-o',
                    '--crawl-offers-from-list',
                    action='store_true',
                    default=False,
                    dest='crawl_offers_from_list',
                    help='Add this argument in case you not only want to parse '
                         'the job offers list, but also crawl the job offers '
                         'scrapped from this list.')
parser.add_argument('-s',
                    '--single-job-offers',
                    action='store_true',
                    default=False,
                    dest='single_job_offer',
                    help='Only crawl the first scrapped job offer page url (by '
                         'default, every job offers listed on the page list '
                         'are crawled).')
parser.add_argument('-p',
                    '--job-page-url',
                    default=None,
                    dest='job_offer_page_url',
                    help='An url pointing to a job offer web page you want to '
                         'test against the job page parser of the specified '
                         'spider class. Cannot be used simultaneously with -l.')


def check_spider_classpath(arguments):
    err_msg = ''
    module_name = ''
    class_name = ''
    try:
        module_name = '.'.join(arguments.spider.split('.')[:-1])
        class_name = arguments.spider.split('.')[-1]

        spider_module = import_module(module_name)
        getattr(spider_module, class_name)
    except ValueError:
        err_msg = 'empty module name'
    except ImportError:
        err_msg = 'module [%s] does not exist' % module_name
    except AttributeError:
        err_msg = 'class [%s] does not exist in module [%s]' % (class_name,
                                                                module_name)
    finally:
        if err_msg:
            print 'The specified spider classpath: [%s], is wrong: %s.' % (
                arguments.spider, err_msg)
            exit()


def check_arguments(arguments):
    check_spider_classpath(arguments)

    if not arguments.debug_crawling:
        if arguments.job_list_page_url \
                or arguments.recursive_crawling \
                or arguments.crawl_offers_from_list \
                or arguments.single_job_offer \
                or arguments.job_offer_page_url:
            print "You shouldn't use debugging parameters without the " \
                  "debugging flag: -d or --debug-crawling.\n"
            parser.print_help()
            exit()

    if arguments.job_list_page_url and arguments.job_offer_page_url:
        print '-l and -p are mutually exclusive.\n'
        parser.print_help()
        exit()

    if arguments.job_offer_page_url and arguments.recursive_crawling:
        print '-r cannot be used with -p.\n'
        parser.print_help()
        exit()

    if arguments.job_offer_page_url and arguments.crawl_offers_from_list:
        print '-o cannot be used with -p.\n'
        parser.print_help()
        exit()

    if arguments.job_offer_page_url and arguments.single_job_offer:
        print '-s cannot be used with -p.\n'
        parser.print_help()
        exit()

if __name__ == '__main__':
    parsed_args = parser.parse_args()
    connector = StdOutputConnector()

    check_arguments(parsed_args)

    debugging_options = None

    if parsed_args.debug_crawling:
        debugging_options = {}
        if parsed_args.job_list_page_url:
            debugging_options['job_list_crawling'] = {
                'url': parsed_args.job_list_page_url,
                'recursive': parsed_args.recursive_crawling,
                'single_job_offer': parsed_args.single_job_offer,
                'crawl_offers_from_list': parsed_args.crawl_offers_from_list
            }
        elif parsed_args.job_offer_page_url:
            debugging_options['job_offer_crawling'] = {
                'url': parsed_args.job_offer_page_url
            }
        else:
            debugging_options['job_list_crawling'] = {
                'recursive': parsed_args.recursive_crawling,
                'single_job_offer': parsed_args.single_job_offer,
                'crawl_offers_from_list': parsed_args.crawl_offers_from_list
            }

    spider = crawl_from_class_name(
        spider_class_path=parsed_args.spider,
        connector=connector,
        spider_error_callback=stdout_error_callback,
        debugging_options=debugging_options
    )

    job_offers = spider.get_connector().get_jobs()

    num_job_offers = len(job_offers)

    if num_job_offers > 0:
        for job in job_offers:
            print('DETAILS FOR %s:' % job['url'])
            print(json.dumps(job.to_dict(), sort_keys=True, indent=4))

    result_analysis_msg = 'Crawling operations are now over. ' \
                          'Here are some results for you: ' \
                          '%s job offer(s) have been scrapped.' % num_job_offers

    if parsed_args.single_job_offer:
        single_job_offer_msg = \
            'Bear in mind that you used -s (or --single-job-offer), meaning ' \
            'that only one job offer has been crawled per job list pages. If ' \
            'you want to crawl every job offers on the list pages, just ' \
            'remove this argument from your call to this script.'
        result_analysis_msg = '%s\n%s' % (result_analysis_msg,
                                          single_job_offer_msg)

    print result_analysis_msg
