#!/usr/bin/env python

# -*- coding: utf-8 -*-
import json
import os
import sys

current_dir = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(current_dir, '../../'))


from pyjobs_crawlers.run import crawl_from_class_name, StdOutputConnector, stdout_error_callback


if __name__ == '__main__':
    connector = StdOutputConnector()
    spider = crawl_from_class_name(
            spider_class_name=sys.argv[1],
            connector=connector,
            spider_error_callback=stdout_error_callback
    )

    jobs = spider.get_connector().get_jobs()

    if len(jobs):
        for job in jobs:
            print('DETAILS FOR %s:' % job['url'])
            print(json.dumps(job.to_dict(), sort_keys=True, indent=4))

    print('TERMINATED: %d job(s) found' % len(jobs))
